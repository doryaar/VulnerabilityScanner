import { Scraper } from "./Scraper";

export class Spider {

  scraper: Scraper;
  targetLinks: string[] = [];
  ignoreList: string[] = ['#', '?~logout', 'calendar.php?']; // Default tags for OWASPBWA

  constructor(private targetUrl: string, session: any, ignoreList?: string[]) {
    this.scraper = new Scraper(session);
    this.targetLinks.push(targetUrl);
    if(ignoreList) this.ignoreList = ignoreList;
  }

  isInIgnoreList(href: string): boolean {
    for(const item of this.ignoreList) {
      if(href.includes(item)) return true;
    }
    return false;
  }

  async crawl(url = this.targetUrl): Promise<string[]> {
    const hrefTags = await this.scraper.extractLinksFrom(url);
    for (const href of hrefTags) {
      if (!href || this.isInIgnoreList(href)) continue;
      const link = (new URL(href, url)).href;
      const isSameHost = link.includes(this.targetUrl);
      const isNewLink = !this.targetLinks.includes(link);
      if (isSameHost && isNewLink) {
        console.log('Scrap: ', link);
        this.targetLinks.push(link);
        await this.crawl(link);
      }
    }
    return this.targetLinks;
  }
}
